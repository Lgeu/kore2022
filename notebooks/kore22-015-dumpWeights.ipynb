{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67855a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "if KAGGLE:\n",
    "    #!pip install kaggle-environments -U\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4cd495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8376aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=cpu\n",
      "n_cpu=16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "from time import time, sleep\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Envirionment\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device={device}\")\n",
    "\n",
    "n_cpu = !lscpu | grep ^CPU\\(s\\):\n",
    "n_cpu = int(n_cpu[0].split()[-1])\n",
    "print(f\"n_cpu={n_cpu}\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Directory settings\n",
    "\n",
    "if KAGGLE:\n",
    "    TEMP_DIR = Path(\"../temp\")\n",
    "    STORAGE_DIR = Path()\n",
    "    PROJECTS_DIR = Path()\n",
    "else:\n",
    "    TEMP_DIR = STORAGE_DIR = Path(\"./015\")\n",
    "    PROJECTS_DIR = Path(\"../../\")\n",
    "\n",
    "if not TEMP_DIR.exists():\n",
    "    print(f\"mkdir {TEMP_DIR}\")\n",
    "    TEMP_DIR.mkdir()\n",
    "if not STORAGE_DIR.exists():\n",
    "    print(f\"mkdir {STORAGE_DIR}\")\n",
    "    STORAGE_DIR.mkdir()\n",
    "\n",
    "sys.path.append(str(STORAGE_DIR))\n",
    "sys.path.append(str(PROJECTS_DIR / \"kore2022\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d13b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd96ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GLOBAL_FEATURES = 9\n",
    "N_SHIPYARD_FEATURES = 32800\n",
    "\n",
    "class NNUE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_feature_encoder = nn.Linear(N_GLOBAL_FEATURES, 256)\n",
    "        self.embedding = nn.EmbeddingBag(N_SHIPYARD_FEATURES + 1, 256, mode=\"sum\", padding_idx=N_SHIPYARD_FEATURES)\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.value_decoder = nn.Linear(256, 1)\n",
    "        self.type_decoder = nn.Linear(256, 4)\n",
    "        self.n_ships_decoder = nn.ModuleList([\n",
    "            nn.Linear(256, 12),  # [1, 10]\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.Linear(256, 32),\n",
    "        ])\n",
    "        self.relative_position_decoder = nn.ModuleList([\n",
    "            None,\n",
    "            nn.Linear(256, 448),  # [0, 441)\n",
    "            nn.Linear(256, 448),\n",
    "            nn.Linear(256, 448),\n",
    "        ])\n",
    "        self.n_steps_decoder = nn.ModuleList([\n",
    "            None,\n",
    "            nn.Linear(256, 24),  # [1, 21]\n",
    "            None,\n",
    "            None,\n",
    "        ])\n",
    "        self.direction_decoder = nn.ModuleList([\n",
    "            None,\n",
    "            None,\n",
    "            nn.Linear(256, 4),\n",
    "            nn.Linear(256, 4),\n",
    "        ])\n",
    "    \n",
    "    def dump(self, file):\n",
    "        # file: filname or file pointer\n",
    "        if isinstance(file, str):\n",
    "            f = open(file, \"wb\")\n",
    "        elif hasattr(file, \"write\"):\n",
    "            f = file\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        def write(params):\n",
    "            f.write(params.detach().cpu().numpy().ravel().tobytes())\n",
    "        \n",
    "        write(self.global_feature_encoder.weight)\n",
    "        write(self.global_feature_encoder.bias)\n",
    "        write(self.embedding.weight)\n",
    "        write(self.fc1.weight)\n",
    "        write(self.fc1.bias)\n",
    "        write(self.fc2.weight)\n",
    "        write(self.fc2.bias)\n",
    "        write(self.value_decoder.weight)\n",
    "        write(self.value_decoder.bias)\n",
    "        write(self.type_decoder.weight)\n",
    "        write(self.type_decoder.bias)\n",
    "        \n",
    "        # Spawn\n",
    "        write(self.n_ships_decoder[0].weight)\n",
    "        write(self.n_ships_decoder[0].bias)\n",
    "        \n",
    "        # Move\n",
    "        write(self.n_ships_decoder[1].weight)\n",
    "        write(self.n_ships_decoder[1].bias)\n",
    "        write(self.relative_position_decoder[1].weight)\n",
    "        write(self.relative_position_decoder[1].bias)\n",
    "        write(self.n_steps_decoder[1].weight)\n",
    "        write(self.n_steps_decoder[1].bias)\n",
    "        \n",
    "        # Attack\n",
    "        write(self.n_ships_decoder[2].weight)\n",
    "        write(self.n_ships_decoder[2].bias)\n",
    "        write(self.relative_position_decoder[2].weight)\n",
    "        write(self.relative_position_decoder[2].bias)\n",
    "        write(self.direction_decoder[2].weight)\n",
    "        write(self.direction_decoder[2].bias)\n",
    "        \n",
    "        # Convert\n",
    "        write(self.n_ships_decoder[3].weight)\n",
    "        write(self.n_ships_decoder[3].bias)\n",
    "        write(self.relative_position_decoder[3].weight)\n",
    "        write(self.relative_position_decoder[3].bias)\n",
    "        write(self.direction_decoder[3].weight)\n",
    "        write(self.direction_decoder[3].bias)\n",
    "        \n",
    "        \n",
    "        if isinstance(file, str):\n",
    "            f.close()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        shipyard_features,\n",
    "        global_features,\n",
    "        target_values,\n",
    "        target_action_types,\n",
    "        target_action_n_ships,  # quantized\n",
    "        target_action_relative_position,\n",
    "        target_action_n_steps,\n",
    "        target_action_direction,\n",
    "    ):\n",
    "        batch_size = shipyard_features.size(0)\n",
    "        shipyard_features[shipyard_features == -100] = N_SHIPYARD_FEATURES\n",
    "        \n",
    "        # [batch_size, N_GLOBAL_FEATURES], [batch_size, 512] -> [batch_size, 256]\n",
    "        x = self.global_feature_encoder(global_features) + self.embedding(shipyard_features)\n",
    "        x = F.leaky_relu(x, 1.0 / 64.0)\n",
    "        \n",
    "        # [batch_size, 256]\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x, 1.0 / 64.0)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x, 1.0 / 64.0)\n",
    "        \n",
    "        # [batch_size, 256] -> [batch_size]\n",
    "        value = self.value_decoder(x).squeeze(1)\n",
    "        # [batch_size, 256] -> [batch_size, 4]\n",
    "        action_type = self.type_decoder(x)\n",
    "        \n",
    "        specific_predictions = []\n",
    "        for i in range(4):\n",
    "            # [batch_size, 256] -> [n_action_data, 256]\n",
    "            xi = x[target_action_types == i]\n",
    "            n_action_data = len(xi)\n",
    "#             if n_action_data == 0:\n",
    "#                 specific_predictions.append([\n",
    "#                     0, None, None, None, None\n",
    "#                 ])\n",
    "            # [n_action_data, 256] -> [n_action_data, ??]\n",
    "            n_ships = self.n_ships_decoder[i](xi)\n",
    "            # [n_action_data, 256] -> [n_action_data, ??]\n",
    "            relative_position = None if self.relative_position_decoder[i] is None else self.relative_position_decoder[i](xi)\n",
    "            # [n_action_data, 256] -> [n_action_data, ??]\n",
    "            n_steps = None if self.n_steps_decoder[i] is None else self.n_steps_decoder[i](xi)\n",
    "            # [n_action_data, 256] -> [n_action_data, ??]\n",
    "            direction = None if self.direction_decoder[i] is None else self.direction_decoder[i](xi)\n",
    "            \n",
    "            specific_predictions.append([\n",
    "                n_action_data, n_ships, relative_position, n_steps, direction\n",
    "            ])\n",
    "        \n",
    "        # === loss computation ===\n",
    "        \n",
    "        value_loss = F.binary_cross_entropy_with_logits(value, target_values, reduction=\"sum\")\n",
    "        type_loss = F.cross_entropy(action_type, target_action_types, reduction=\"sum\")\n",
    "        loss = value_loss * 10.0 + type_loss\n",
    "        ACTION_LOSS_WEIGHTS = [1.0, 1.0, 5.0, 25.0]\n",
    "        \n",
    "        specific_losses = []\n",
    "        for i in range(4):\n",
    "            n_action_data, n_ships, relative_position, n_steps, direction = specific_predictions[i]\n",
    "            indices = target_action_types == i\n",
    "            \n",
    "            n_ships_loss = F.cross_entropy(n_ships, target_action_n_ships[indices], reduction=\"sum\")\n",
    "            action_loss = n_ships_loss.clone()\n",
    "            \n",
    "            if relative_position is None:\n",
    "                relative_position_loss = None\n",
    "            else:\n",
    "                relative_position_loss = F.cross_entropy(relative_position, target_action_relative_position[indices], reduction=\"sum\")\n",
    "                action_loss += relative_position_loss\n",
    "            \n",
    "            if n_steps is None:\n",
    "                n_steps_loss = None\n",
    "            else:\n",
    "                n_steps_loss = F.cross_entropy(n_steps, target_action_n_steps[indices], reduction=\"sum\")\n",
    "                action_loss += n_steps_loss\n",
    "            \n",
    "            if direction is None:\n",
    "                direction_loss = None\n",
    "            else:\n",
    "                direction_loss = F.cross_entropy(direction, target_action_direction[indices], reduction=\"sum\")\n",
    "                action_loss += direction_loss\n",
    "            \n",
    "            loss += ACTION_LOSS_WEIGHTS[i] * action_loss\n",
    "            \n",
    "            specific_losses.append([\n",
    "                n_action_data, n_ships_loss, relative_position_loss, n_steps_loss, direction_loss\n",
    "            ])\n",
    "        \n",
    "        loss *= 1 / batch_size\n",
    "        \n",
    "        return (value, action_type, specific_predictions), (value_loss, type_loss, specific_losses), loss\n",
    "\n",
    "model = NNUE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659bc6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #checkpoint_name = \"01340000\"\n",
    "# checkpoint_name = \"02180000\"\n",
    "# dict_checkpoint = torch.load(f\"010/checkpoint_{checkpoint_name}.pt\", map_location=\"cpu\")\n",
    "# model.load_state_dict(dict_checkpoint[\"state_dict\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb1225d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkpoint_name = \"02720000\"\n",
    "checkpoint_name = \"03000000\"\n",
    "dict_checkpoint = torch.load(f\"024/checkpoint_{checkpoint_name}.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(dict_checkpoint[\"state_dict\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "848dbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dump(str(STORAGE_DIR / f\"parameters_{checkpoint_name}.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a2f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
